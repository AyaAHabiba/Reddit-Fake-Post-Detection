{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "YphuA9SjyZmC",
   "metadata": {
    "id": "YphuA9SjyZmC"
   },
   "source": [
    "# There is a problem 😧😧😧😧😧😧\n",
    "# What is it ?????? & What we should dooo???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dnjYhvUJy_y6",
   "metadata": {
    "id": "dnjYhvUJy_y6"
   },
   "source": [
    "**The input:**\n",
    "\n",
    "it representing Reddit Fake Post Detection ( by looking at its title)\n",
    "\n",
    "conisting of only one feature that represent the title of the posts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JVbXa1-f0MCq",
   "metadata": {
    "id": "JVbXa1-f0MCq"
   },
   "source": [
    "**The output:**\n",
    "\n",
    "it is the label which specify if the post in Reddit is fack or not( prediction of the probability (0-1, float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x4ojpgk70LyH",
   "metadata": {
    "id": "x4ojpgk70LyH"
   },
   "source": [
    "**Data mining function:**\n",
    "\n",
    "1.Generalization and Summarization\n",
    "\n",
    "2.Association and Correlation\n",
    "\n",
    "3.Clustering\n",
    "\n",
    "4.Outlier/Anomaly Analysis\n",
    "\n",
    "5.Classification & Prediction\n",
    "\n",
    "5.Time and Ordering \n",
    "\n",
    "6.Structure and Network Analysis\n",
    "\n",
    "**The data mining in this problem requires Classification & Prediction After cleaning the data by:**\n",
    "\n",
    "*   handeling any missing data\n",
    "*   removing useless data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SQMmdJmm0Lks",
   "metadata": {
    "id": "SQMmdJmm0Lks"
   },
   "source": [
    "**The impact**\n",
    "if we use the data without cleaning,it will give us a model with low accuracy which will not learn well.\n",
    "\n",
    "The real-life impact of this model is represent in limitting the number of fack posts that could be created on Reddit site by detecting it and distinguish it from the real ones just by looking at the title.\n",
    "\n",
    "\n",
    "if we solve it, we will destroy the fack messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aIiZ8ZEK3mnJ",
   "metadata": {
    "id": "aIiZ8ZEK3mnJ"
   },
   "source": [
    "**The challenges:**\n",
    "\n",
    "* Noise data (useless data)\n",
    "* Missing data\n",
    "* label that contain niether 1 nor 0\n",
    "* the preprocessing of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdU-p2pz4MEG",
   "metadata": {
    "id": "fdU-p2pz4MEG"
   },
   "source": [
    "**The ideal solution:**\n",
    "\n",
    "1. clean and preprocess the data \n",
    "2. we will try word and character vectorizer and see which one will give high performance \n",
    "3. we found that word vectorizer will give best performance\n",
    "4. trying different models with different hyperparaameters to get the ideal solution\n",
    "5. the best model that gives higher performance score was LogisticRegression with validation set followed by LogisticRegression with word vectorizer then XGBClassifier on word with_validation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xRvtDirN5k-K",
   "metadata": {
    "id": "xRvtDirN5k-K"
   },
   "source": [
    "**The experimental protocol used and how was it carried out:**\n",
    "After loading the data and cleaning and preprocessing it, different experimental protocol are used through different trials:\n",
    "\n",
    "using validation set to measure the performance while using any classifier with word-level vectorizer\n",
    "using hyperparamter search method RandomizedSearchCV / or BayesSearchCV\n",
    "\n",
    "we measure the perormance using (roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1GYtpwU4L5K",
   "metadata": {
    "id": "n1GYtpwU4L5K"
   },
   "source": [
    "**The preprocessing steps are used:**\n",
    "* we remove the labels with values of not 0 or 1\n",
    "* we apply the character and the word using stemmer(we remove the lower case,remove the stopping word,remove single letter chars,\n",
    "convert all whitespaces to single wspace, punctuation and stemm, remove html tags, remove text that contain very low number of characters (consider it useless data), Keep only ASCII + European Chars and whitespace, no digits\n",
    "* apply feature creation with TF-IDF (word vectorizer & character vectorizer)\n",
    "* we try the the apply again on the character and word but this time we use the lemmatizer (with the same steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YWr7sld95kyf",
   "metadata": {
    "id": "YWr7sld95kyf"
   },
   "source": [
    "# Import the libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb00ba4",
   "metadata": {
    "id": "edb00ba4",
    "outputId": "af5bb51d-2978-4be3-fb2b-71205378f63b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in c:\\programdata\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.2.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.10.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (21.10.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.23.5)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mportlib-metadata (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mportlib-metadata (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mportlib-metadata (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mportlib-metadata (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mportlib-metadata (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mportlib-metadata (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35197a90",
   "metadata": {
    "id": "35197a90",
    "outputId": "1a86f955-63e7-4908-cb00-35fbebc0d588"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.3/dist/panel.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.3/dist/panel.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import nltk \n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# some seeting for pandas and hvplot\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 300\n",
    "pd.options.display.max_colwidth = 100\n",
    "np.set_printoptions(threshold=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c955640",
   "metadata": {
    "id": "4c955640"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf72526",
   "metadata": {
    "id": "0cf72526"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17jPvozQ_tAC",
   "metadata": {
    "id": "17jPvozQ_tAC"
   },
   "source": [
    "# **Data Cleaning and Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qCqm0l_I-UhE",
   "metadata": {
    "id": "qCqm0l_I-UhE"
   },
   "source": [
    "# Load the data(train & test) and read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569673e",
   "metadata": {
    "id": "e569673e",
    "outputId": "6e4e8324-5a08-4069-f2b8-55a8e14098d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265723</td>\n",
       "      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284269</td>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207715</td>\n",
       "      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551106</td>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8584</td>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>70046</td>\n",
       "      <td>Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>189377</td>\n",
       "      <td>Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>93486</td>\n",
       "      <td>Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>140950</td>\n",
       "      <td>Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>34509</td>\n",
       "      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  \\\n",
       "0      265723   \n",
       "1      284269   \n",
       "2      207715   \n",
       "3      551106   \n",
       "4        8584   \n",
       "...       ...   \n",
       "59995   70046   \n",
       "59996  189377   \n",
       "59997   93486   \n",
       "59998  140950   \n",
       "59999   34509   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0      A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n",
       "1      British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n",
       "2      In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n",
       "3      Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n",
       "4      Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n",
       "...                                                                                                    ...   \n",
       "59995                Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)   \n",
       "59996                Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back   \n",
       "59997                Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no   \n",
       "59998                Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)   \n",
       "59999                Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep   \n",
       "\n",
       "       label  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "59995      0  \n",
       "59996      1  \n",
       "59997      0  \n",
       "59998      0  \n",
       "59999      1  \n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading our dataset\n",
    "df_train = pd.read_csv(\"xy_train.csv\")  #Reading our train set \n",
    "df_test = pd.read_csv(\"x_test.csv\")   #Reading our test set\n",
    "df_train #printing our train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nNd9s_zl-avy",
   "metadata": {
    "id": "nNd9s_zl-avy"
   },
   "source": [
    "# Remove the id as it is not a feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ceb803",
   "metadata": {
    "id": "57ceb803"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"x_test.csv\")   #Reading our test set\n",
    "\n",
    "df_test_id=df_test['id']\n",
    "df_test_id\n",
    "df_test.drop(columns=['id'],axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb8236",
   "metadata": {
    "id": "1edb8236"
   },
   "outputs": [],
   "source": [
    "df_train.drop(columns=['id'],axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08dde5b",
   "metadata": {
    "id": "a08dde5b",
    "outputId": "c00cd873-044f-4abf-a497-18b5715e2676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    60000 non-null  object\n",
      " 1   label   60000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info() #to get the information of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JpKYqIY9-8Le",
   "metadata": {
    "id": "JpKYqIY9-8Le"
   },
   "source": [
    "Calculate the nulls to know if there is any missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93eb5a1",
   "metadata": {
    "id": "a93eb5a1",
    "outputId": "f8411817-8779-4e98-aa25-50ef0379d034"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8hWtsPn9_XVO",
   "metadata": {
    "id": "8hWtsPn9_XVO"
   },
   "source": [
    "Here ,we want only the 0s & 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e274a",
   "metadata": {
    "id": "651e274a",
    "outputId": "e79d3d14-8aa7-4cfd-c872-6b335ffc770d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59768 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "0      A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n",
       "1      British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n",
       "2      In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n",
       "3      Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n",
       "4      Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n",
       "...                                                                                                    ...   \n",
       "59995                Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)   \n",
       "59996                Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back   \n",
       "59997                Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no   \n",
       "59998                Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)   \n",
       "59999                Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep   \n",
       "\n",
       "       label  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "59995      0  \n",
       "59996      1  \n",
       "59997      0  \n",
       "59998      0  \n",
       "59999      1  \n",
       "\n",
       "[59768 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows that contain label = 2 \n",
    "df_train = df_train[(df_train[\"label\"] != 2)]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afdda2",
   "metadata": {
    "id": "82afdda2"
   },
   "source": [
    "# Apply stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g30K1_gSBMqX",
   "metadata": {
    "id": "g30K1_gSBMqX"
   },
   "source": [
    "There is a rule-based process of word form conversion where word-suffixes are truncated irrespective of whether the root word is an actual word in the language dictionary so we applied stemming to convert tokens to their root form\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2oJbL_ItAL6F",
   "metadata": {
    "id": "2oJbL_ItAL6F"
   },
   "source": [
    "steps:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        * remove any html tags (< /br> often found)\n",
    "\n",
    "        * Keep only ASCII + European Chars and whitespace, no digits\n",
    "\n",
    "        * remove single letter chars\n",
    "\n",
    "        * convert all whitespaces (tabs etc.) to single wspace\n",
    "           if not for embedding (but e.g. tdf-idf):\n",
    "\n",
    "        * all lowercase\n",
    "\n",
    "        * remove stopwords, punctuation and stemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e828e37",
   "metadata": {
    "id": "8e828e37",
    "outputId": "028ff3db-b230-4398-83ab-67d5ee2563e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def clean_text(text, for_embedding=False):\n",
    "    \"\"\" steps:\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4XM8-54AwQA",
   "metadata": {
    "id": "s4XM8-54AwQA"
   },
   "source": [
    "we want to clean the comments from the train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3f8cb",
   "metadata": {
    "id": "27a3f8cb",
    "outputId": "3d3dd861-ad63-4855-e0fc-e012b69eaa01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAB\\AppData\\Local\\Temp\\ipykernel_15128\\3569877927.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"text_clean\"] = df_train.loc[df_train[\"text\"].str.len() > 20, \"text\"]\n",
      "C:\\Users\\LAB\\AppData\\Local\\Temp\\ipykernel_15128\\3569877927.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"text_clean\"] = df_train[\"text_clean\"].map(\n"
     ]
    }
   ],
   "source": [
    "# Clean Comments\n",
    "df_train[\"text_clean\"] = df_train.loc[df_train[\"text\"].str.len() > 20, \"text\"]\n",
    "df_train[\"text_clean\"] = df_train[\"text_clean\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uGrJlC0zkmGQ",
   "metadata": {
    "id": "uGrJlC0zkmGQ"
   },
   "source": [
    "Here, we drop any nulls or spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb822a7e",
   "metadata": {
    "id": "bb822a7e"
   },
   "outputs": [],
   "source": [
    "# Drop when any of x missing\n",
    "df_train = df_train[(df_train[\"text_clean\"] != \"\") & (df_train[\"text_clean\"] != \"null\")]\n",
    "\n",
    "df_train = df_train.dropna(axis=\"index\", ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahtieedkkuqg",
   "metadata": {
    "id": "ahtieedkkuqg"
   },
   "source": [
    "Show the data after removing allllllll of the previous noise 😠😠(after preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c82ee",
   "metadata": {
    "id": "850c82ee",
    "outputId": "f51014c5-40d8-4fe7-b357-b09690f6578f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n",
       "      <td>0</td>\n",
       "      <td>group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n",
       "      <td>0</td>\n",
       "      <td>british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n",
       "      <td>0</td>\n",
       "      <td>goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n",
       "      <td>0</td>\n",
       "      <td>obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59753</th>\n",
       "      <td>Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)</td>\n",
       "      <td>0</td>\n",
       "      <td>finish sniper simo yh invas finland ussr color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59754</th>\n",
       "      <td>Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back</td>\n",
       "      <td>1</td>\n",
       "      <td>nigerian princ scam took kansa man year later get back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59755</th>\n",
       "      <td>Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no</td>\n",
       "      <td>0</td>\n",
       "      <td>safe smoke marijuana pregnanc surpris answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59756</th>\n",
       "      <td>Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)</td>\n",
       "      <td>0</td>\n",
       "      <td>julius caesar upon realiz everyon room knife except bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59757</th>\n",
       "      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep</td>\n",
       "      <td>1</td>\n",
       "      <td>jeff bridg releas leep tape new album design help fall asleep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59758 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "0      A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n",
       "1      British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n",
       "2      In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n",
       "3      Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n",
       "4      Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n",
       "...                                                                                                    ...   \n",
       "59753                Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)   \n",
       "59754                Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back   \n",
       "59755                Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no   \n",
       "59756                Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)   \n",
       "59757                Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep   \n",
       "\n",
       "       label  \\\n",
       "0          0   \n",
       "1          0   \n",
       "2          0   \n",
       "3          0   \n",
       "4          0   \n",
       "...      ...   \n",
       "59753      0   \n",
       "59754      1   \n",
       "59755      0   \n",
       "59756      0   \n",
       "59757      1   \n",
       "\n",
       "                                                                                                text_clean  \n",
       "0      group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...  \n",
       "1      british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...  \n",
       "2      goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...  \n",
       "3      happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...  \n",
       "4      obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...  \n",
       "...                                                                                                    ...  \n",
       "59753                                                       finish sniper simo yh invas finland ussr color  \n",
       "59754                                               nigerian princ scam took kansa man year later get back  \n",
       "59755                                                         safe smoke marijuana pregnanc surpris answer  \n",
       "59756                                               julius caesar upon realiz everyon room knife except bc  \n",
       "59757                                        jeff bridg releas leep tape new album design help fall asleep  \n",
       "\n",
       "[59758 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laupGSQMlJ86",
   "metadata": {
    "id": "laupGSQMlJ86"
   },
   "source": [
    "We will apply the same thing on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a3ddc",
   "metadata": {
    "id": "001a3ddc"
   },
   "outputs": [],
   "source": [
    "# Clean texts for test data\n",
    "df_test[\"text_clean\"] = df_test.loc[df_test[\"text\"].str.len() > 0, \"text\"]\n",
    "df_test[\"text_clean\"] = df_test[\"text_clean\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1820d",
   "metadata": {
    "id": "53a1820d",
    "outputId": "292b10e6-7976-481d-ccda-84f9d8b59a85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stargazer</td>\n",
       "      <td>stargaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yeah</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PD: Phoenix car thief gets instructions from YouTube video</td>\n",
       "      <td>pd phoenix car thief get instruct youtub video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As Trump Accuses Iran, He Has One Problem: His Own Credibility</td>\n",
       "      <td>trump accus iran one problem credibl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Believers\" - Hezbollah 2011</td>\n",
       "      <td>believ hezbollah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59146</th>\n",
       "      <td>Bicycle taxi drivers of New Delhi</td>\n",
       "      <td>bicycl taxi driver new delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59147</th>\n",
       "      <td>Trump blows up GOP's formula for winning House races</td>\n",
       "      <td>trump blow gop formula win hous race</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59148</th>\n",
       "      <td>Napoleon returns from his exile on the island of Elba. (March 1815), Colourised</td>\n",
       "      <td>napoleon return exil island elba march colouris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59149</th>\n",
       "      <td>Deep down he always wanted to be a ballet dancer</td>\n",
       "      <td>deep alway want ballet dancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59150</th>\n",
       "      <td>Toddler miraculously survives 6-story fall landing on car</td>\n",
       "      <td>toddler miracul surviv stori fall land car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  text  \\\n",
       "0                                                                           stargazer    \n",
       "1                                                                                 yeah   \n",
       "2                           PD: Phoenix car thief gets instructions from YouTube video   \n",
       "3                       As Trump Accuses Iran, He Has One Problem: His Own Credibility   \n",
       "4                                                         \"Believers\" - Hezbollah 2011   \n",
       "...                                                                                ...   \n",
       "59146                                                Bicycle taxi drivers of New Delhi   \n",
       "59147                             Trump blows up GOP's formula for winning House races   \n",
       "59148  Napoleon returns from his exile on the island of Elba. (March 1815), Colourised   \n",
       "59149                                 Deep down he always wanted to be a ballet dancer   \n",
       "59150                        Toddler miraculously survives 6-story fall landing on car   \n",
       "\n",
       "                                            text_clean  \n",
       "0                                              stargaz  \n",
       "1                                                 yeah  \n",
       "2       pd phoenix car thief get instruct youtub video  \n",
       "3                 trump accus iran one problem credibl  \n",
       "4                                     believ hezbollah  \n",
       "...                                                ...  \n",
       "59146                     bicycl taxi driver new delhi  \n",
       "59147             trump blow gop formula win hous race  \n",
       "59148  napoleon return exil island elba march colouris  \n",
       "59149                    deep alway want ballet dancer  \n",
       "59150       toddler miracul surviv stori fall land car  \n",
       "\n",
       "[59151 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbac396",
   "metadata": {
    "id": "ebbac396"
   },
   "outputs": [],
   "source": [
    "df_train_copy=df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iV02l38MlQIG",
   "metadata": {
    "id": "iV02l38MlQIG"
   },
   "source": [
    "We count the words (the frequency of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731adf04",
   "metadata": {
    "id": "731adf04",
    "outputId": "ce227c2b-1bcf-4e6c-b801-b923184a5115"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one         3285\n",
       "like        3128\n",
       "new         2998\n",
       "look        2847\n",
       "color       2737\n",
       "man         2729\n",
       "get         2602\n",
       "trump       2578\n",
       "say         2347\n",
       "peopl       2316\n",
       "use         2307\n",
       "first       2248\n",
       "make        2227\n",
       "old         2226\n",
       "time        2027\n",
       "poster      2000\n",
       "found       1999\n",
       "day         1935\n",
       "war         1858\n",
       "post        1648\n",
       "world       1570\n",
       "work        1531\n",
       "show        1513\n",
       "us          1506\n",
       "american    1504\n",
       "take        1491\n",
       "life        1482\n",
       "psbattl     1470\n",
       "help        1442\n",
       "go          1420\n",
       "state       1409\n",
       "back        1369\n",
       "two         1364\n",
       "school      1345\n",
       "see         1329\n",
       "photo       1324\n",
       "made        1314\n",
       "right       1311\n",
       "save        1308\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.models import NumeralTickFormatter\n",
    "# Word Frequency of most common words\n",
    "word_freq = pd.Series(\" \".join(df_train_copy[\"text_clean\"]).split()).value_counts()\n",
    "word_freq[1:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3GvUR-Olledr",
   "metadata": {
    "id": "3GvUR-Olledr"
   },
   "source": [
    "list most uncommon words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a8a6b",
   "metadata": {
    "id": "643a8a6b",
    "outputId": "35ed3eb7-d625-4f03-9e64-8988d9398c70"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robertshaw</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sawzal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battersea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flung</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sekou</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hiroshig</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dirtier</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noseslid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>danta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>angriff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>delusion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wane</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>undament</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>miku</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hatsun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nfler</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hicock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mccall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wahr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  freq\n",
       "0   robertshaw     1\n",
       "1       sawzal     1\n",
       "2          vle     1\n",
       "3    battersea     1\n",
       "4        flung     1\n",
       "5        sekou     1\n",
       "6     hiroshig     1\n",
       "7      dirtier     1\n",
       "8     noseslid     1\n",
       "9        danta     1\n",
       "10     angriff     1\n",
       "11    delusion     1\n",
       "12        wane     1\n",
       "13    undament     1\n",
       "14        miku     1\n",
       "15      hatsun     1\n",
       "16       nfler     1\n",
       "17      hicock     1\n",
       "18      mccall     1\n",
       "19        wahr     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq[-20:].reset_index(name=\"freq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9LC_Ym3blkZW",
   "metadata": {
    "id": "9LC_Ym3blkZW"
   },
   "source": [
    "we just check the missing after preprocessing(on train & test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13e8af",
   "metadata": {
    "id": "4c13e8af",
    "outputId": "85d6c20c-9eb0-43f0-fc42-887475d3444c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          0\n",
       "label         0\n",
       "text_clean    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking missing values\n",
    "df_train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568676c",
   "metadata": {
    "id": "c568676c",
    "outputId": "6f099be5-f487-4e76-9003-503d63859ea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          0\n",
       "text_clean    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking missing values\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b3a6c",
   "metadata": {
    "id": "db3b3a6c"
   },
   "source": [
    "# Spliting data to X & Y then print the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd61504",
   "metadata": {
    "id": "3cd61504",
    "outputId": "e67b0de3-1c95-4758-e679-20ebb17e8ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59758,)\n",
      "(59758,)\n",
      "(59151,)\n"
     ]
    }
   ],
   "source": [
    "# Sample data - 25% of data to test set\n",
    "x_train = df_train_copy[\"text_clean\"]\n",
    "y_train = df_train_copy[\"label\"]\n",
    "x_test=df_test[\"text_clean\"]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419422bf",
   "metadata": {
    "id": "419422bf"
   },
   "source": [
    "# Split data to train & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d006b",
   "metadata": {
    "id": "579d006b"
   },
   "outputs": [],
   "source": [
    "x_train1, x_val, y_train1, y_val = train_test_split(x_train, y_train, train_size = 0.8, stratify = y_train, shuffle=True, random_state = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7gFoURdnl66i",
   "metadata": {
    "id": "7gFoURdnl66i"
   },
   "source": [
    "Create a list where train data indices are -1 and validation data indices are 0\n",
    "\n",
    "use the list to create PredefinedSplit(for validation set as we will use it sooner ⁉)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6a969",
   "metadata": {
    "id": "f6c6a969",
    "outputId": "8bfb4ffd-5107-4e87-f434-79edd3a9edaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 44.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "# X_tr (new training set), X_train\n",
    "split_index = [-1 if x in x_train1.index else 0 for x in x_train.index]\n",
    "pd = PredefinedSplit(test_fold = split_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9840edb",
   "metadata": {
    "id": "f9840edb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "LUqUoZ0XnAP0",
   "metadata": {
    "id": "LUqUoZ0XnAP0"
   },
   "source": [
    "# **Apply fleature creation with TF-IDF** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nfJrSk325qtL",
   "metadata": {
    "id": "nfJrSk325qtL"
   },
   "source": [
    "as classification models cannot deal with text data directly, we need to convert our it to a numeric representation. so, we will see it using tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc0487",
   "metadata": {
    "id": "78bc0487"
   },
   "source": [
    "# XGB Classifier with character vectorization using BayesSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0037e0d",
   "metadata": {
    "id": "e0037e0d"
   },
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1662a7f",
   "metadata": {
    "id": "c1662a7f",
    "outputId": "71e4469e-74e0-4ee4-ade5-b04d16d35cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "best score OrderedDict([('my_classifier__max_depth', 1000), ('my_classifier__n_estimators', 200), ('tfidf__max_df', 0.3), ('tfidf__min_df', 64)])\n",
      "CPU times: total: 4min 11s\n",
      "Wall time: 7min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe = Pipeline([\n",
    "                 (\"tfidf\", TfidfVectorizer(analyzer=\"char\")),\n",
    "                 ('my_classifier', XGBClassifier())\n",
    "                 ])\n",
    "\n",
    "params = {\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    'my_classifier__n_estimators': [100, 150, 200],  \n",
    "    'my_classifier__max_depth':[300, 500, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "pipe_clf = BayesSearchCV(\n",
    "    pipe, params, cv=5, verbose=1, n_jobs=2,\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "pipe_clf.fit(x_train, y_train)\n",
    "\n",
    "print('best score {}'.format(pipe_clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Ar6IV_An54w",
   "metadata": {
    "id": "6Ar6IV_An54w"
   },
   "source": [
    "* we make a pipline contains {our vectorizer:tfidf (applid on the character),and the model(classifier):XGBClassifier}\n",
    "* we played wuth the hyperparameters :\n",
    "1. max_df->tfidf\n",
    "2. min_df->tfidf\n",
    "3. no. of estimator->classifier\n",
    "4. max_depth ->classifier\n",
    "* in the BayesSearchCV -> {crossvalidation=5, no. of itreration =50, and our metric ->roc_auc}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ECjojaObpeS4",
   "metadata": {
    "id": "ECjojaObpeS4"
   },
   "source": [
    "the best score \n",
    "1. 'my_classifier__max_depth'-> 1000\n",
    "2. 'my_classifier__n_estimators'-> 200\n",
    "3. 'tfidf__max_df'-> 0.3\n",
    "4. 'tfidf__min_df'-> 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iDSfp_JlnOi5",
   "metadata": {
    "id": "iDSfp_JlnOi5"
   },
   "source": [
    "we print the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb4e28",
   "metadata": {
    "id": "b2cb4e28",
    "outputId": "af6b4157-8ed0-431b-f195-d7d2cbbcd6d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.5310297304897816\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_clf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WlyJPokppx-X",
   "metadata": {
    "id": "WlyJPokppx-X"
   },
   "source": [
    "# Conclision:\n",
    " XGBClassifier on character vectorization is toooooo baddddd 😞😞😞😞😞😞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481b99a",
   "metadata": {
    "id": "c481b99a"
   },
   "outputs": [],
   "source": [
    "import pandas as pand\n",
    "submission = pand.DataFrame()\n",
    "submission['id'] = df_test_id\n",
    "\n",
    "submission['label'] = pipe_clf.predict_proba(df_test['text_clean'])[:,1]\n",
    "\n",
    "submission.to_csv('baysien_search_xg_ch.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80773bff",
   "metadata": {
    "id": "80773bff"
   },
   "source": [
    "# XGB Classifier with word vectorization using BayesSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48840f22",
   "metadata": {
    "id": "48840f22",
    "outputId": "7aa2bdd6-ffc7-4faf-b277-2b2f67662aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "best score OrderedDict([('my_classifier__max_depth', 248), ('my_classifier__n_estimators', 184), ('tfidf__max_df', 0.3), ('tfidf__min_df', 21)])\n",
      "CPU times: total: 5h 7min 40s\n",
      "Wall time: 56min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe1 = Pipeline([\n",
    "                 (\"tfidf\", TfidfVectorizer(analyzer=\"word\",ngram_range=(1, 3), norm=\"l2\")),\n",
    "                 ('my_classifier', XGBClassifier())\n",
    "                 ])\n",
    "\n",
    "params1 = {\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    'my_classifier__n_estimators': [100, 200],  \n",
    "    'my_classifier__max_depth':[200, 300]\n",
    "}\n",
    "\n",
    "\n",
    "pipe_clf1 = BayesSearchCV(\n",
    "    pipe1, params1, cv=3, verbose=1, n_jobs=1,\n",
    " \n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "pipe_clf1.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print('best score {}'.format(pipe_clf1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SK2w67jKqGJI",
   "metadata": {
    "id": "SK2w67jKqGJI"
   },
   "source": [
    "* we make a pipline contains {our vectorizer:tfidf (applid on the word),and the model(classifier):XGBClassifier}\n",
    "* we played wuth the hyperparameters :\n",
    "1. max_df->tfidf\n",
    "2. min_df->tfidf\n",
    "3. no. of estimator->classifier\n",
    "4. max_depth ->classifier\n",
    "* in the BayesSearchCV -> {crossvalidation=3, no. of itreration =10, and our metric ->roc_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bmNRguLMqF23",
   "metadata": {
    "id": "bmNRguLMqF23"
   },
   "source": [
    "best score OrderedDict\n",
    "1. 'my_classifier__max_depth'-> 248\n",
    "2. 'my_classifier__n_estimators'-> 184\n",
    "3. 'tfidf__max_df'-> 0.3\n",
    "4. 'tfidf__min_df'-> 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xJWeKD-WnppR",
   "metadata": {
    "id": "xJWeKD-WnppR"
   },
   "source": [
    "we print the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7fd41",
   "metadata": {
    "id": "a1a7fd41",
    "outputId": "11045b0e-f3f2-4aed-a913-dc773a899715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8289744592700323\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_clf1.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SuWB8_aTql_b",
   "metadata": {
    "id": "SuWB8_aTql_b"
   },
   "source": [
    "# Conclusion:\n",
    "XGBClassifier on word vectorization is much moreeeeeeee better than on character 🤭🤭🤭🤭🤭🤭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157acff",
   "metadata": {
    "id": "5157acff"
   },
   "outputs": [],
   "source": [
    "import pandas as pand\n",
    "submission = pand.DataFrame()\n",
    "submission['id'] = df_test_id\n",
    "\n",
    "submission['label'] = pipe_clf1.predict_proba(df_test['text_clean'])[:,1]\n",
    "\n",
    "submission.to_csv('baysien_search_xg_wd.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30196efe",
   "metadata": {
    "id": "30196efe"
   },
   "source": [
    "# XGB Classifier with validation set on word using RandomSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6CWrM8-Etj69",
   "metadata": {
    "id": "6CWrM8-Etj69"
   },
   "source": [
    "we will use our predefined split internally to determine which sample belongs to the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e170a",
   "metadata": {
    "id": "eb4e170a",
    "outputId": "e009045d-79fe-476c-b4e4-29d4a4958a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 5 candidates, totalling 5 fits\n",
      "best score {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 15, 'tfidf__max_df': 0.3, 'my_classifier__n_estimators': 150, 'my_classifier__max_depth': 200}\n"
     ]
    }
   ],
   "source": [
    "pipe_val = Pipeline([\n",
    "                 (\"tfidf\", TfidfVectorizer(analyzer=\"word\")),\n",
    "                 ('my_classifier', XGBClassifier())\n",
    "                 ])\n",
    "\n",
    "params_val = {\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    'my_classifier__n_estimators': [100, 150, 200],  \n",
    "    'my_classifier__max_depth':[200, 300, 500]\n",
    "}\n",
    "pipe_clf_val = RandomizedSearchCV(\n",
    "    pipe_val, params_val, cv=pd, verbose=1, n_jobs=2,\n",
    "    n_iter=5,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "pipe_clf_val.fit(x_train, y_train)\n",
    "\n",
    "print('best score {}'.format(pipe_clf_val.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SJxSgV07r7LP",
   "metadata": {
    "id": "SJxSgV07r7LP"
   },
   "source": [
    "* we make a pipline contains {our vectorizer:tfidf (applid on the validation set on word),and the model(classifier):XGBClassifier}\n",
    "* we played wuth the hyperparameters :\n",
    "1. max_df->tfidf\n",
    "2. min_df->tfidf\n",
    "3. no. of estimator->classifier\n",
    "4. max_depth ->classifier\n",
    "5. ngram_range_df->tfidf\n",
    "* in the RandomizedSearchCV -> {crossvalidation=pd(which the predifined function that we did before for\n",
    " validation set), no. of itreration =5, and our metric ->roc_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eJ_vJkn1sqFD",
   "metadata": {
    "id": "eJ_vJkn1sqFD"
   },
   "source": [
    "best score \n",
    "1. 'tfidf__ngram_range'-> (1, 2)\n",
    "2. 'tfidf__min_df'-> 15\n",
    "3. 'tfidf__max_df'-> 0.3\n",
    "4. 'my_classifier__n_estimators'-> 150\n",
    "5. 'my_classifier__max_depth'-> 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfb2e8",
   "metadata": {
    "id": "8cbfb2e8",
    "outputId": "7f344e50-295b-40cd-8b06-1d3fe1b49a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8520841487993641\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_clf_val.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wzJM2JhotCRR",
   "metadata": {
    "id": "wzJM2JhotCRR"
   },
   "source": [
    "# Conclusion:\n",
    "XGBClassifier gives more and more accuracy \n",
    "which is the best one until now\n",
    "🙉🙉🙉🙉🙉🙉🙉🙉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62df60",
   "metadata": {
    "id": "be62df60"
   },
   "outputs": [],
   "source": [
    "import pandas as pand\n",
    "submission = pand.DataFrame()\n",
    "submission['id'] = df_test_id\n",
    "\n",
    "submission['label'] = pipe_clf_val.predict_proba(df_test['text_clean'])[:,1]\n",
    "\n",
    "submission.to_csv('baysien_search_xg_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb3faa",
   "metadata": {
    "id": "0adb3faa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "792914c6",
   "metadata": {
    "id": "792914c6"
   },
   "source": [
    "# Logistic Regression with character vectorization using BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13764bcf",
   "metadata": {
    "id": "13764bcf",
    "outputId": "334a2e7a-f562-4b59-bc0f-c795c8c63487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "best score OrderedDict([('my_classifier__C', 0.0001), ('my_classifier__penalty', 'l2'), ('my_classifier__solver', 'liblinear'), ('tfidf__max_df', 0.3), ('tfidf__min_df', 42)])\n",
      "CPU times: total: 4min 46s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe_lg_ch = Pipeline([\n",
    "                 (\"tfidf\", TfidfVectorizer(analyzer=\"char\")),\n",
    "                 ('my_classifier', LogisticRegression())\n",
    "                 ])\n",
    "\n",
    "params_lg_ch = {\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    'my_classifier__penalty' : ['l1', 'l2'],\n",
    "    'my_classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'my_classifier__solver' : ['liblinear']\n",
    "}\n",
    "\n",
    "\n",
    "pipe_clf_lg_ch = BayesSearchCV(\n",
    "    pipe_lg_ch, params_lg_ch, cv=5, verbose=1, n_jobs=2,\n",
    "\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "pipe_clf_lg_ch.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print('best score {}'.format(pipe_clf_lg_ch.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bLTwdokst7Rm",
   "metadata": {
    "id": "bLTwdokst7Rm"
   },
   "source": [
    "* we make a pipline contains {our vectorizer:tfidf (applid on the character),and the model(classifier):LogisticRegression}\n",
    "* we played wuth the hyperparameters :\n",
    "1. max_df->tfidf\n",
    "2. min_df->tfidf\n",
    "3. penalty->classifier\n",
    "4. C ->classifier\n",
    "5. solver->classifier\n",
    "\n",
    "* in the BayesSearchCV -> {crossvalidation=5, no. of itreration =50, and our metric ->roc_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sDdQeWbJt7Kc",
   "metadata": {
    "id": "sDdQeWbJt7Kc"
   },
   "source": [
    "best score\n",
    "1. 'my_classifier__C'-> 0.0001 \n",
    "2. 'my_classifier__penalty'-> 'l2'\n",
    "3. 'my_classifier__solver'-> 'liblinear'\n",
    "4. 'tfidf__max_df'-> 0.3\n",
    "5. 'tfidf__min_df'-> 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b224b82",
   "metadata": {
    "id": "9b224b82",
    "outputId": "b724f365-847c-4871-b3d7-319a27302a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.5334006757097621\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_clf_lg_ch.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nF8L3Cy4vg3f",
   "metadata": {
    "id": "nF8L3Cy4vg3f"
   },
   "source": [
    "# Conclusion:\n",
    "Again, character vectorizer gives very pooooooor accuracy 😞😞😞😞😞😞 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7dab46",
   "metadata": {
    "id": "de7dab46"
   },
   "outputs": [],
   "source": [
    "import pandas as pand\n",
    "submission = pand.DataFrame()\n",
    "submission['id'] = df_test_id\n",
    "\n",
    "submission['label'] = pipe_clf_lg_ch.predict_proba(df_test['text_clean'])[:,1]\n",
    "\n",
    "submission.to_csv('baysien_search_lg_ch.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac3b71",
   "metadata": {
    "id": "00ac3b71"
   },
   "source": [
    "# Logistic Regression with word vectorization using BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8731108",
   "metadata": {
    "id": "b8731108",
    "outputId": "0f8c664f-7202-4cbe-ccc1-7719fc806a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "best score OrderedDict([('my_classifier__C', 1.623776739188721), ('my_classifier__penalty', 'l2'), ('my_classifier__solver', 'liblinear'), ('tfidf__max_df', 0.3), ('tfidf__min_df', 13)])\n",
      "CPU times: total: 4min 34s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe_lg_wd = Pipeline([\n",
    "                 (\"tfidf\", TfidfVectorizer(analyzer=\"word\")),\n",
    "                 ('my_classifier', LogisticRegression())\n",
    "                 ])\n",
    "\n",
    "params_lg_wd = {\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    'my_classifier__penalty' : ['l1', 'l2'],\n",
    "    'my_classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'my_classifier__solver' : ['liblinear']\n",
    "}\n",
    "\n",
    "\n",
    "pipe_clf_lg_wd = BayesSearchCV(\n",
    "    pipe_lg_wd, params_lg_ch, cv=5, verbose=1, n_jobs=2,\n",
    "    # number of random trials\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "# here we still use X_train; but the grid search model\n",
    "# will use our predefined split internally to determine \n",
    "# which sample belongs to the validation set\n",
    "pipe_clf_lg_wd.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print('best score {}'.format(pipe_clf_lg_wd.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2wW_kteSwCUc",
   "metadata": {
    "id": "2wW_kteSwCUc"
   },
   "source": [
    "* we make a pipline contains {our vectorizer:tfidf (applid on the word),and the model(classifier):LogisticRegression}\n",
    "* we played wuth the hyperparameters :\n",
    "1. max_df->tfidf\n",
    "2. min_df->tfidf\n",
    "3. penalty->classifier\n",
    "4. C ->classifier\n",
    "5. solver->classifier\n",
    "\n",
    "* in the BayesSearchCV -> {crossvalidation=5, no. of itreration =50, and our metric ->roc_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o13uE8lnwNQv",
   "metadata": {
    "id": "o13uE8lnwNQv"
   },
   "source": [
    "best score \n",
    "1. 'my_classifier__C'-> 1.623776739188721\n",
    "2. 'my_classifier__penalty'-> 'l2'\n",
    "3. 'my_classifier__solver'-> 'liblinear'\n",
    "4. 'tfidf__max_df'-> 0.3\n",
    "5. 'tfidf__min_df'-> 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09360d4b",
   "metadata": {
    "id": "09360d4b",
    "outputId": "2af86488-af06-4710-cbf3-550d6dfbff7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8600119182982701\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_clf_lg_wd.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aNRASukvwdS8",
   "metadata": {
    "id": "aNRASukvwdS8"
   },
   "source": [
    "# Conclusion:\n",
    "Here, again \n",
    "the word vecrorization gives the best accuracy (until now)\n",
    "Logistic regression with word vecrorization is better than with XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5281d1",
   "metadata": {
    "id": "6a5281d1"
   },
   "outputs": [],
   "source": [
    "import pandas as pand\n",
    "submission = pand.DataFrame()\n",
    "submission['id'] = df_test_id\n",
    "\n",
    "submission['label'] = pipe_clf_lg_wd.predict_proba(df_test['text_clean'])[:,1]\n",
    "\n",
    "submission.to_csv('baysien_search_lg_wd.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95685d38",
   "metadata": {
    "id": "95685d38"
   },
   "source": [
    "# Logistic Regression with validation on the word using BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PD2xcGIWxSkN",
   "metadata": {
    "id": "PD2xcGIWxSkN"
   },
   "source": [
    "we will use our predefined split internally to determine which sample belongs to the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662fe31",
   "metadata": {
    "id": "9662fe31",
    "outputId": "50c27a37-9883-48fb-bbb4-4a62e1df7bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "best score OrderedDict([('my_classifier__C', 0.615848211066026), ('my_classifier__penalty', 'l2'), ('my_classifier__solver', 'liblinear'), ('tfidf__max_df', 0.3), ('tfidf__min_df', 15)])\n",
      "CPU times: total: 3.64 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe_lg_val = Pipeline([\n",
    "                 (\"tfidf\", TfidfVectorizer(analyzer=\"word\")),\n",
    "                 ('my_classifier', LogisticRegression())\n",
    "                 ])\n",
    "\n",
    "params_lg_val = {\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    'my_classifier__penalty' : ['l1', 'l2'],\n",
    "    'my_classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'my_classifier__solver' : ['liblinear']\n",
    "}\n",
    "\n",
    "\n",
    "pipe_clf_lg_val = BayesSearchCV(\n",
    "    pipe_lg_val, params_lg_val, cv=pd, verbose=1, n_jobs=2,\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "\n",
    "pipe_clf_lg_val.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print('best score {}'.format(pipe_clf_lg_val.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PFzUpu7gxalg",
   "metadata": {
    "id": "PFzUpu7gxalg"
   },
   "source": [
    "* we make a pipline contains {our vectorizer:tfidf (applid on the validation set with word),and the model(classifier):LogisticRegression}\n",
    "* we played wuth the hyperparameters :\n",
    "1. max_df->tfidf\n",
    "2. min_df->tfidf\n",
    "3. penalty->classifier\n",
    "4. C ->classifier\n",
    "5. solver->classifier\n",
    "\n",
    "* in the BayesSearchCV -> {crossvalidation=pd, no. of itreration =10, and our metric ->roc_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HFrBogCtyK1u",
   "metadata": {
    "id": "HFrBogCtyK1u"
   },
   "source": [
    "best score \n",
    "1. 'my_classifier__C -> 0.615848211066026 \n",
    "2. 'my_classifier__penalty'-> 'l2'\n",
    "3. 'my_classifier__solver'-> 'liblinear'\n",
    "4. 'tfidf__max_df'-> 0.3\n",
    "5. 'tfidf__min_df'-> 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdd1bc",
   "metadata": {
    "id": "edfdd1bc",
    "outputId": "632d1bef-b48b-4350-9266-c5a66bf39106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8684416427604911\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_clf_lg_val.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67wcRogHybcH",
   "metadata": {
    "id": "67wcRogHybcH"
   },
   "source": [
    "#Conclusion:\n",
    "Voilaaaaaaaaaaaa 😇😇😇😇😇\n",
    "Logistic regression on validation set of the word ranked the 1st \n",
    "👏👏👏👏👏👏👏👏👏👏 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae156a",
   "metadata": {
    "id": "d6ae156a"
   },
   "outputs": [],
   "source": [
    "import pandas as pand\n",
    "submission = pand.DataFrame()\n",
    "submission['id'] = df_test_id\n",
    "\n",
    "submission['label'] = pipe_clf_lg_val.predict_proba(df_test['text_clean'])[:,1]\n",
    "\n",
    "submission.to_csv('baysien_search_lg_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd19bb3",
   "metadata": {
    "id": "dbd19bb3"
   },
   "source": [
    "# Apply lemmatizer (another type of preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sehDQaYPzGGI",
   "metadata": {
    "id": "sehDQaYPzGGI"
   },
   "source": [
    "There is a rule-based process of word form conversion where word-suffixes are truncated irrespective of whether the root word is an actual word in the language dictionary so we applied lemmatizer\n",
    " to convert tokens to their root form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gVajUJMmzeYt",
   "metadata": {
    "id": "gVajUJMmzeYt"
   },
   "source": [
    "steps:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        * remove any html tags (< /br> often found)\n",
    "\n",
    "        * Keep only ASCII + European Chars and whitespace, no digits\n",
    "\n",
    "        * remove single letter chars\n",
    "\n",
    "        * convert all whitespaces (tabs etc.) to single wspace\n",
    "           if not for embedding (but e.g. tdf-idf):\n",
    "\n",
    "        * all lowercase\n",
    "\n",
    "        * remove stopwords, punctuation and stemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc627d",
   "metadata": {
    "id": "79dc627d",
    "outputId": "e3982d6a-31f5-4769-b0bd-d57bb2f7ac78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "limitization = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def clean_text_limit(text, for_embedding=False):\n",
    "    \"\"\" steps:\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            limitization.lemmatize(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jOCNo1l40FVZ",
   "metadata": {
    "id": "jOCNo1l40FVZ"
   },
   "source": [
    "Clean Comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86272926",
   "metadata": {
    "id": "86272926"
   },
   "outputs": [],
   "source": [
    "df_train[\"text_clean_lim\"] = df_train.loc[df_train[\"text\"].str.len() > 20, \"text\"]\n",
    "df_train[\"text_clean_lim\"] = df_train[\"text_clean_lim\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LJpPOxEG0WJg",
   "metadata": {
    "id": "LJpPOxEG0WJg"
   },
   "source": [
    "Remove the null or the spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac5029",
   "metadata": {
    "id": "7cac5029"
   },
   "outputs": [],
   "source": [
    "df_train = df_train[(df_train[\"text_clean_lim\"] != \"\") & (df_train[\"text_clean_lim\"] != \"null\")]\n",
    "\n",
    "df_train = df_train.dropna(axis=\"index\", ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a488b5",
   "metadata": {
    "id": "a0a488b5"
   },
   "outputs": [],
   "source": [
    "# Clean texts for test data\n",
    "df_test[\"text_clean_lim\"] = df_test.loc[df_test[\"text\"].str.len() > 0, \"text\"]\n",
    "df_test[\"text_clean_lim\"] = df_test[\"text_clean_lim\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D4yXMm9f0NeC",
   "metadata": {
    "id": "D4yXMm9f0NeC"
   },
   "source": [
    "# Split the data to x & Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f77046",
   "metadata": {
    "id": "03f77046",
    "outputId": "434a9bcc-d08a-432f-ce79-b6415712ee17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59758,)\n",
      "(59758,)\n",
      "(59151,)\n"
     ]
    }
   ],
   "source": [
    "# Sample data - 25% of data to test set\n",
    "x_train1 = df_train[\"text_clean_lim\"]\n",
    "y_train1 = df_train[\"label\"]\n",
    "x_test1=df_test[\"text_clean_lim\"]\n",
    "\n",
    "print(x_train1.shape)\n",
    "print(y_train1.shape)\n",
    "print(x_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25cf83",
   "metadata": {
    "id": "7a25cf83"
   },
   "source": [
    "# RandomForest to word using BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e454aa",
   "metadata": {
    "id": "57e454aa",
    "outputId": "67fd2d17-1267-4b4c-d51e-946aacfdb904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score OrderedDict([('my_classifier__max_depth', 100), ('my_classifier__n_estimators', 40), ('tfidf__max_df', 0.3), ('tfidf__min_df', 41)])\n",
      "CPU times: total: 22 s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# feature creation and modelling in a single function\n",
    "pipe2 = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\")), \n",
    "                  (\"my_classifier\", RandomForestClassifier())])\n",
    "\n",
    "# define parameter space to test # runtime 35min\n",
    "params2 = {\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    \"my_classifier__n_estimators\": [30, 40, 50],  \n",
    "    \"my_classifier__max_depth\":[30,50,100]       \n",
    "}\n",
    "# it is quite slow so we do 4 for now\n",
    "pipe_clf2 = BayesSearchCV(\n",
    "    pipe2, params2, n_jobs=-1, scoring=\"roc_auc\", n_iter=10)\n",
    "pipe_clf2.fit(x_train1, y_train1)\n",
    "\n",
    "\n",
    "print('best score {}'.format(pipe_clf2.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8jhxdxpy0mWN",
   "metadata": {
    "id": "8jhxdxpy0mWN"
   },
   "source": [
    "* we make a pipline contains {our vectorizer:tfidf (applid on word),and the model(classifier):RandomForest}\n",
    "* we played wuth the hyperparameters :\n",
    "1. max_df->tfidf\n",
    "2. min_df->tfidf\n",
    "3. mx_depth->classifier\n",
    "\n",
    "* in the BayesSearchCV -> { no. of itreration =10,\n",
    " and our metric ->roc_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SW-DGfQu0mFQ",
   "metadata": {
    "id": "SW-DGfQu0mFQ"
   },
   "source": [
    "best score \n",
    "1. 'my_classifier__max_depth'-> 100\n",
    "2. 'my_classifier__n_estimators'-> 40 \n",
    "3. 'tfidf__max_df'-> 0.3\n",
    "4. 'tfidf__min_df'-> 41\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0eab60",
   "metadata": {
    "id": "8a0eab60",
    "outputId": "dce38988-4900-4b85-cd60-665879d55313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8121652483370484\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_clf2.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TKyP4dg71T3g",
   "metadata": {
    "id": "TKyP4dg71T3g"
   },
   "source": [
    "# Conclusion:\n",
    "RandomForest to the word vectorization gives good accuracy but not the best 😲😲😲😲"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be770a28",
   "metadata": {
    "id": "be770a28"
   },
   "source": [
    "# Logistic Regression with validation on word using BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OfepRvs-11Ld",
   "metadata": {
    "id": "OfepRvs-11Ld"
   },
   "source": [
    "we will use our predefined split internally to determine which sample belongs to the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92934303",
   "metadata": {
    "id": "92934303",
    "outputId": "bd3a2292-a79c-4549-cf1d-740e28d73b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "best score OrderedDict([('my_classifier__C', 29.763514416313132), ('my_classifier__penalty', 'l2'), ('my_classifier__solver', 'liblinear'), ('tfidf__max_df', 0.3), ('tfidf__min_df', 22)])\n",
      "CPU times: total: 4.09 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe_lg_val1 = Pipeline([\n",
    "                 (\"tfidf\", TfidfVectorizer(analyzer=\"word\")),\n",
    "                 ('my_classifier', LogisticRegression())\n",
    "                 ])\n",
    "\n",
    "params_lg_val1 = {\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    'my_classifier__penalty' : ['l1', 'l2'],\n",
    "    'my_classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'my_classifier__solver' : ['liblinear']\n",
    "}\n",
    "\n",
    "\n",
    "pipe_clf_lg_val1 = BayesSearchCV(\n",
    "    pipe_lg_val1, params_lg_val1, cv=pd, verbose=1, n_jobs=2,\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "\n",
    "\n",
    "pipe_clf_lg_val1.fit(x_train1, y_train1)\n",
    "\n",
    "\n",
    "print('best score {}'.format(pipe_clf_lg_val1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8Fjm_9XB18gd",
   "metadata": {
    "id": "8Fjm_9XB18gd"
   },
   "source": [
    "* we make a pipline contains {our vectorizer:tfidf (applid validation set of word),and the model(classifier):Logistic regression}\n",
    "* we played wuth the hyperparameters :\n",
    "1. max_df->tfidf\n",
    "2. min_df->tfidf\n",
    "3. panlty->classifier\n",
    "4. C->classifier\n",
    "5. solver->classifier\n",
    "\n",
    "* in the BayesSearchCV -> {cv=pd no. of itreration =10,\n",
    " and our metric ->roc_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RSys_0Bc2hEC",
   "metadata": {
    "id": "RSys_0Bc2hEC"
   },
   "source": [
    "best score \n",
    "1. 'my_classifier__C'-> 29.763514416313132\n",
    "2. 'my_classifier__penalty'-> 'l2'\n",
    "3. 'my_classifier__solver'-> 'liblinear'\n",
    "4. 'tfidf__max_df'-> 0.3\n",
    "5. 'tfidf__min_df'-> 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fcc093",
   "metadata": {
    "id": "93fcc093",
    "outputId": "f7bddcb8-3bca-450b-d8aa-5923f9a2bfce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8596996309711372\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_clf_lg_val1.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yv1SbNIL2wqm",
   "metadata": {
    "id": "yv1SbNIL2wqm"
   },
   "source": [
    "# Conclusion:\n",
    "Still Logistic regression is the best model until now \n",
    "and it applied the best on stemmer with validation set on the word\n",
    "👏👏👏👏👏👏👏👏👏👏👏👏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827b11c",
   "metadata": {
    "id": "e827b11c"
   },
   "source": [
    "# XGBClassifier on word on validation using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283308f",
   "metadata": {
    "id": "f283308f",
    "outputId": "7e18e607-2b31-4946-f494-8b08d42f909c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 4 candidates, totalling 4 fits\n",
      "best score {'tfidf__min_df': 32, 'tfidf__max_df': 0.3, 'my_classifier__n_estimators': 200, 'my_classifier__max_depth': 300}\n",
      "CPU times: total: 1h 32min 24s\n",
      "Wall time: 17min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe1l = Pipeline([\n",
    "                 (\"tfidf\", TfidfVectorizer(analyzer=\"word\",ngram_range=(1, 3), norm=\"l2\")),\n",
    "                 ('my_classifier', XGBClassifier())\n",
    "                 ])\n",
    "\n",
    "params1l = {\n",
    "#     \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    'my_classifier__n_estimators': [100, 200],  \n",
    "    'my_classifier__max_depth':[200, 300]\n",
    "}\n",
    "\n",
    "\n",
    "pipe_clf1l = RandomizedSearchCV(\n",
    "    pipe1l, params1l, cv=pd, verbose=1, n_jobs=1,\n",
    "    # number of random trials\n",
    "    n_iter=4,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "# here we still use X_train; but the grid search model\n",
    "# will use our predefined split internally to determine \n",
    "# which sample belongs to the validation set\n",
    "pipe_clf1l.fit(x_train1, y_train1)\n",
    "\n",
    "\n",
    "print('best score {}'.format(pipe_clf1l.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ntrIwiI3m-N",
   "metadata": {
    "id": "2ntrIwiI3m-N"
   },
   "source": [
    "* we make a pipline contains {our vectorizer:tfidf (applid validation set of word),and the model(classifier):XGBClassifier}\n",
    "* we played wuth the hyperparameters :\n",
    "1. max_df->tfidf\n",
    "2. min_df->tfidf\n",
    "3. max_depth->classifier\n",
    "4. no.estimator->classifier\n",
    "\n",
    "\n",
    "* in the RandomSearchCV -> {cv=pd no. of itreration =4,\n",
    " and our metric ->roc_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7QqGMbUO3m1X",
   "metadata": {
    "id": "7QqGMbUO3m1X"
   },
   "source": [
    "best score \n",
    "1. 'tfidf__min_df'-> 32\n",
    "2. 'tfidf__max_df'-> 0.3\n",
    "3. 'my_classifier__n_estimators'-> 200\n",
    "5. 'my_classifier__max_depth'-> 300\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76671790",
   "metadata": {
    "id": "76671790",
    "outputId": "38bfd3cd-bd7f-4759-9015-836519dafd83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8470363970520615\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_clf1l.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pKFAqylw4G5k",
   "metadata": {
    "id": "pKFAqylw4G5k"
   },
   "source": [
    "#Conclusion:\n",
    "XGBClassifier gives good but not best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59039b",
   "metadata": {
    "id": "5d59039b"
   },
   "outputs": [],
   "source": [
    "import pandas as pand\n",
    "submission = pand.DataFrame()\n",
    "submission['id'] = df_test_id\n",
    "\n",
    "submission['label'] = pipe_clf1l.predict_proba(df_test['text_clean'])[:,1]\n",
    "\n",
    "submission.to_csv('baysien_search_lg_val_l.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w6KZ3ibw4Tup",
   "metadata": {
    "id": "w6KZ3ibw4Tup"
   },
   "source": [
    "#Conclusion:\n",
    "the overall obesrvation:\n",
    "* RandomForest gives lower accuracy (it is not good fit for our data set)\n",
    "* XGBClassifier gives higher accuracy than RandomForest but not the best \n",
    "* Logistic regression gives the best accuracy\n",
    "* lemitizer or stemmer did not make huge differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HRXVwS-Z45BI",
   "metadata": {
    "id": "HRXVwS-Z45BI"
   },
   "source": [
    "# The winner is the Logistic Regression 👏👏👏👏👏👏👏👏👏👏👏👏👏👏👏\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GZKvMxB05ETM",
   "metadata": {
    "id": "GZKvMxB05ETM"
   },
   "source": [
    "#🌈 What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XeLTK7Au6QW1",
   "metadata": {
    "id": "XeLTK7Au6QW1"
   },
   "source": [
    "Word n-gram model : represent unique word sequence of length n as\n",
    "feature .\n",
    "Character n-gram model : represent unique character sequence of length n as\n",
    "feature\n",
    "(in another way: n-gram means a sequence of n words,a continuous sequence of characters of the specified length while Character n-grams splits each word based on characters but word n-grams splits the text based on words)\n",
    "\n",
    "\n",
    "Word n-gram tends to suffer more from the OOV issue as:\n",
    "\n",
    "Out Of Vocabulary (OOV) words refer to the new words(do not exist in the vocabulary) which are faced at testing. Hence, these methods fail in handling OOV words.\n",
    "\n",
    "[link text](https://reader.elsevier.com/reader/sd/pii/S1877042813041918?token=3E1F8BE1DC047E8C3B89A98E9A583DE265B6477EF7E4DC237DACFA7F133D2A53A3592532CD63666A37288E8E65F64116&originRegion=eu-west-1&originCreation=20220314111053)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1IVFVWFy6QMm",
   "metadata": {
    "id": "1IVFVWFy6QMm"
   },
   "source": [
    "#🌈 What is the difference between stop word removal and stemming? Are these techniques language-dependent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O9p0Ium46Pmf",
   "metadata": {
    "id": "O9p0Ium46Pmf"
   },
   "source": [
    "stop words are high frequency words that have small weight and are can not help the retrieval process. \n",
    "\n",
    "Stemming remove morphological affixes from words and these techniques are language dependant.\n",
    "\n",
    "but both are commonly used method in indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZSgYBH8S9OXU",
   "metadata": {
    "id": "ZSgYBH8S9OXU"
   },
   "source": [
    "#🌈 Is tokenization techniques language dependent? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "znuDZtQL9OL0",
   "metadata": {
    "id": "znuDZtQL9OL0"
   },
   "source": [
    "Tokenization is splitting a phrase, sentence, paragraph, or even a text into smaller units, it should know the meaning of each word to split it ,depending on language (so, it is language dependent).\n",
    "There are various tokenization techniques available which can be applicable based on the language and purpose of modeling.\n",
    "\n",
    "Major techniques for tokenizing are:\n",
    "\n",
    "Split, Spacy, Gensim, Regular Expression, NLTK (Natural Language Toolkit) library, Dictionary Based Tokenization, Rule Based Tokenization, Penn TreeBank Tokenization, and many others...\n",
    "\n",
    "\n",
    "[link text](https://medium.com/unpackai/natural-language-processing-tokenization-and-numericalization-63c2df20d917)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dqM5qLSB3sk",
   "metadata": {
    "id": "8dqM5qLSB3sk"
   },
   "source": [
    "# 🌈 What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PWJOm72PB3Wo",
   "metadata": {
    "id": "PWJOm72PB3Wo"
   },
   "source": [
    "\n",
    "\n",
    "In CountVectorizer, we only count the number of a word's times that appears in the document . this ends up in ignoring rare words which may help in processing our data more efficiently.\n",
    "\n",
    "In TfidfVectorizer, we consider all the document (not for each word)weightage of a word , it weights the word counts by a measure of how often they appear in the documents.(it helps us in dealing with most frequent words)\n",
    "\n",
    "\n",
    "TF-IDF is better than Count Vectorizers as it is not only focuses on the frequency of words present in the corpus but also provides the importance of the words. We can then remove  the less important words for analysis.\n",
    "\n",
    "No, it Wouldn't be feasible to use all possible n-grams especially if it is of a significant a size\n",
    "\n",
    "As it depends on the application and the dataset. In some applications, it is important to deal with some combinations of words together as they are together, have a big effect( as sentiment analysis problem)\n",
    "\n",
    "but, TfidfVectorizer and CountVectorizer both are methods for converting text data into vectors as model can process only numerical data.\n",
    "\n",
    "[link text](https://www.quora.com/What-is-the-difference-between-TfidfVectorizer-and-CountVectorizer-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bx918_aI5Qut",
   "metadata": {
    "id": "Bx918_aI5Qut"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
